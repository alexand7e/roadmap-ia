<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Infográfico Transformers - Governo do Piauí</title>
  <meta name="description" content="Arquitetura de Transformers: Self-Attention, Multi-Head e fluxo Encoder-Decoder.">
  <meta name="keywords" content="Transformers, Self-Attention, Encoder, Decoder, IA, Governo do Piauí">
  <meta property="og:title" content="Infográfico: Transformers | Governo do Piauí">
  <meta property="og:description" content="Visual didático da matriz de atenção e o mecanismo Self-Attention.">
  <meta property="og:type" content="website">
  <link rel="canonical" href="/infografico/3">
  <link rel="icon" type="image/png" href="/assets/logo-gov.png">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@600;700;800&family=Source+Sans+3:wght@400;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <script>tailwind.config={theme:{extend:{colors:{piGreen:'#0B6B3A',piYellow:'#F4C430',piBlue:'#0A2540',piGray:'#F2F2F2'},fontFamily:{sans:['"Source Sans 3"','sans-serif'],display:['"Montserrat"','sans-serif']},boxShadow:{'poster':'0 20px 50px -12px rgba(0,0,0,0.25)'}}}};</script>
  <link rel="stylesheet" href="/src/styles/base.css">
</head>
<body class="text-piBlue">
  <div class="infographic-paper shadow-poster w-full max-w-[1200px] bg-white">
    <header class="bg-piGray px-8 py-8 border-b-8 border-piGreen flex flex-col md:flex-row items-center justify-between relative overflow-hidden">
      <div class="absolute top-0 right-0 w-64 h-64 bg-piYellow opacity-10 rounded-full transform translate-x-1/2 -translate-y-1/2"></div>
      <div class="flex items-center gap-4 z-10">
        <a href="/" aria-label="Ir para a página inicial">
          <img src="/assets/logo-gov.png" alt="Governo do Piauí" class="w-16 h-30 object-contain shrink-0">
        </a>
        <div>
          <h2 class="text-piGreen font-bold font-display text-sm tracking-widest uppercase">Governo do Estado do Piauí</h2>
          <h1 class="text-piBlue font-display font-extrabold text-3xl">Transformers</h1>
          <span class="text-piGreen font-semibold text-lg bg-piYellow px-2 text-piBlue inline-block mt-1">Arquitetura Revolucionária</span>
        </div>
      </div>
      <div class="mt-4 md:mt-0 text-center md:text-right z-10 flex items-center gap-4">
        <p class="text-sm text-gray-600 max-w-xs">Self-Attention e multi-head para entender dependências longas.</p>
        <button id="hc-toggle" class="ml-2 px-3 py-2 rounded-md bg-piBlue text-white text-xs font-bold shadow hover:opacity-90" aria-pressed="false" aria-label="Alternar alto contraste">Alto Contraste</button>
      </div>
    </header>
    <main class="p-4 md:p-10">
      <div class="text-center mb-8">
        <span class="bg-piBlue text-white px-6 py-2 rounded-full font-bold uppercase text-sm tracking-wide shadow-md"><i class="fas fa-chart-grid mr-2"></i> Matriz de Atenção</span>
      </div>
      
      <div class="max-w-3xl mx-auto mb-8 text-gray-700 leading-relaxed">
        <p class="mb-4 text-base">
          A arquitetura Transformer, introduzida em 2017, revolucionou o processamento de linguagem natural ao substituir os modelos sequenciais 
          anteriores por um mecanismo de atenção paralela. O componente central é o mecanismo de Self-Attention, que permite que cada palavra 
          em uma frase "preste atenção" em todas as outras palavras simultaneamente, capturando dependências de longo alcance de forma eficiente.
        </p>
        <p class="text-base">
          Diferente das RNNs que processam sequências palavra por palavra, os Transformers processam toda a sequência em paralelo, tornando o 
          treinamento muito mais rápido. O Multi-Head Attention permite que o modelo capture diferentes tipos de relacionamentos (sintáticos, 
          semânticos, etc.) simultaneamente, enquanto o Positional Encoding adiciona informações sobre a ordem das palavras, já que a atenção 
          por si só não possui noção inerente de sequência.
        </p>
      </div>

      <div class="flex justify-center">
        <object data="/assets/svgs/3-transformer-attention.svg" type="image/svg+xml" class="w-full max-w-lg" aria-label="Matriz de Atenção">Matriz de Atenção</object>
      </div>
      <div class="mt-8 grid grid-cols-1 md:grid-cols-3 gap-6">
        <div class="bg-white p-6 rounded-xl shadow-sm border-l-4 border-piGreen"><h4 class="font-display font-bold text-piBlue mb-2">Self-Attention</h4><p class="text-sm text-gray-600">Cada token pondera todos os outros para formar o contexto.</p></div>
        <div class="bg-white p-6 rounded-xl shadow-sm border-l-4 border-piYellow"><h4 class="font-display font-bold text-piBlue mb-2">Multi-Head</h4><p class="text-sm text-gray-600">Cabeças paralelas capturam padrões diferentes do texto.</p></div>
        <div class="bg-white p-6 rounded-xl shadow-sm border-l-4 border-piBlue"><h4 class="font-display font-bold text-piBlue mb-2">Positional Encoding</h4><p class="text-sm text-gray-600">Senoides/cossenoides adicionam noções de ordem aos tokens.</p></div>
      </div>
    </main>
    <footer class="bg-piGreen text-white py-8 text-center border-t-8 border-piYellow relative">
      <div class="container mx-auto px-4">
        <p class="font-display font-bold text-lg italic mb-4">"Dados, inovação e inteligência para transformar o Piauí."</p>
        <div class="flex justify-center items-center gap-2 text-sm opacity-80">
          <span>Governo do Estado do Piauí</span>
          <span>&bull;</span>
          <span>2026</span>
        </div>
        <div class="mt-6 flex justify-center">
          <img src="/assets/capacitia-logo.png" alt="Capacitia" class="h-20 opacity-80 hover:opacity-100 transition-opacity">
        </div>
      </div>
    </footer>
    <nav class="fixed bottom-6 right-6 flex items-center gap-2 z-50">
      <a href="/infografico/2" class="bg-piBlue text-white w-12 h-12 rounded-full flex items-center justify-center font-bold shadow-lg hover:bg-piGreen hover:scale-110 transition-all duration-300" aria-label="Anterior">
        <i class="fas fa-chevron-left"></i>
      </a>
      <a href="/infografico/4" class="bg-piBlue text-white w-12 h-12 rounded-full flex items-center justify-center font-bold shadow-lg hover:bg-piGreen hover:scale-110 transition-all duration-300" aria-label="Próximo">
        <i class="fas fa-chevron-right"></i>
      </a>
    </nav>
  </div>
  <script>
    const btn=document.getElementById('hc-toggle');if(btn){btn.addEventListener('click',()=>{const a=document.body.classList.toggle('hc');btn.setAttribute('aria-pressed',String(a));});}
  </script>
</body>
</html>
